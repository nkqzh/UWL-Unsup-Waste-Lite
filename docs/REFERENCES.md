docs/REFERENCES.md（来源备份）

数据集：

TACO（官网/论文/代码）

https://tacodataset.org/

https://github.com/pedropro/TACO

TrashNet（原始仓库/Kaggle 镜像/Roboflow 版本）

https://github.com/garythung/trashnet

https://www.kaggle.com/datasets/kneroma/tacotrashdataset

MJU-Waste（Sensors 论文数据/镜像）

https://github.com/realwecan/mju-waste

https://datasetninja.com/mju-waste

开放词汇/分割：

GroundingDINO（ECCV 2024） https://github.com/IDEA-Research/GroundingDINO

Grounded-SAM 项目 https://github.com/IDEA-Research/Grounded-Segment-Anything

OWL-ViT（Google）

论文 https://arxiv.org/abs/2205.06230

文档 https://huggingface.co/docs/transformers/en/model_doc/owlvit

SAM 与 SAM 2

https://segment-anything.com/

https://ai.meta.com/sam2/ ； https://github.com/facebookresearch/sam2

自监督表征：

DINOv2（TMLR 2024）

https://arxiv.org/abs/2304.07193

https://github.com/facebookresearch/dinov2

MAE（CVPR 2022）

https://arxiv.org/abs/2111.06377

https://openaccess.thecvf.com/content/CVPR2022/papers/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper.pdf

学生检测器：

YOLO11（Ultralytics） https://docs.ultralytics.com/models/yolo11/

RT-DETRv2（CVPR 2024 / HF 文档） https://huggingface.co/docs/transformers/en/model_doc/rt_detr_v2

官方 RT-DETR（lyuwenyu） https://github.com/lyuwenyu/RT-DETR

端侧推理：

ONNX Runtime 安装与文档

https://onnxruntime.ai/docs/install/

https://pypi.org/project/onnxruntime/

上述链接均为可公开访问的官方/权威来源，便于论文引用与复现。